{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fbedf55-396e-4d9a-9fc5-f0acc2cde064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from nltk import corpus as corpiss\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92671451-7b6e-4676-8105-488a37630e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opener(filename):\n",
    "    with open(filename) as f:\n",
    "        contents = f.readlines()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60efca11-d6ff-410d-877f-fa135eaea1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'category': '\"X\"s & \"O\"s',\n",
       "  'question': 'This 1797 imbroglio began when 3 French agents demanded a huge bribe from U.S. diplomats',\n",
       "  'value': 2000,\n",
       "  'answer': 'the XYZ Affair'},\n",
       " {'category': 'THE SOLAR SYSTEM',\n",
       "  'question': 'Objects that pass closer to the sun than Mercury have been named for this mythological figure',\n",
       "  'value': None,\n",
       "  'answer': 'Icarus'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('./jeopardy.json')\n",
    "corpus = json.load(f)\n",
    "for i,item in enumerate(corpus):\n",
    "    item['question'] = item['question'][1:-1]\n",
    "    if re.search('href',item['question']):\n",
    "        del corpus[i]\n",
    "        continue\n",
    "    del item['air_date']\n",
    "    del item['round']\n",
    "    del item['show_number']\n",
    "    if item['value'] is None:\n",
    "        continue\n",
    "    item['value'] = int(item['value'][1:].replace(',',''))\n",
    "corpus[50:52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "705d65f6-f7c8-41b7-9d35-3a84cdcc6493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84aacc5383d4f358439cb73831645db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b510593b8f4fc69731f1f0fa2f5cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862b93009b0246388ceefe3582b5f523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f950a2b0f947888ce71a16aa876b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cb20276-02b9-4fbc-8f56-ef5a95a07eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MEDICINE & HISTORY',\n",
       " 'THE LYNDON JOHNSON YEARS',\n",
       " 'DAWN',\n",
       " 'CENTRAL PARK ENTERTAINMENT',\n",
       " 'WHAT A DELICIOUS DISH']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categories = set([i['category']for i in corpus])\n",
    "# list(categories)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a080416-82da-4616-8a17-b8292f00819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(text,type='question'):\n",
    "#     if type=='question':\n",
    "#         text=text[1:-1]\n",
    "#     text=re.sub('</a>','',re.sub(r'<a href=\".*\">','',re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', text))).split()\n",
    "#     text = [word.lower() for word in text]\n",
    "#     return text\n",
    "# # tokenize(corpus[0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eec6fc4f-14dc-4f29-8274-e1ab4967434b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['for',\n",
       "  'the',\n",
       "  'last',\n",
       "  '8',\n",
       "  'years',\n",
       "  'of',\n",
       "  'his',\n",
       "  'life',\n",
       "  ',',\n",
       "  'galileo',\n",
       "  'was',\n",
       "  'under',\n",
       "  'house',\n",
       "  'arrest',\n",
       "  'for',\n",
       "  'espousing',\n",
       "  'this',\n",
       "  \"man's\",\n",
       "  'theory'],\n",
       " ['no',\n",
       "  '.',\n",
       "  '2',\n",
       "  ':',\n",
       "  '1912',\n",
       "  'olympian',\n",
       "  ';',\n",
       "  'football',\n",
       "  'star',\n",
       "  'at',\n",
       "  'carlisle',\n",
       "  'indian',\n",
       "  'school',\n",
       "  ';',\n",
       "  '6',\n",
       "  'mlb',\n",
       "  'seasons',\n",
       "  'with',\n",
       "  'the',\n",
       "  'reds',\n",
       "  ',',\n",
       "  'giants',\n",
       "  '&',\n",
       "  'braves']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# questions = [j for j in [tokenize(i['question']) for i in corpus]]\n",
    "# questions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95bafd35-acb2-47c9-8066-a2bf8ca10ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def opener(questions):\n",
    "#     out = []\n",
    "#     for idx,question in enumerate(questions):\n",
    "#         for word in question:\n",
    "#             out.append(word)\n",
    "#     return out\n",
    "# questions_joined = ' '.join(opener(questions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b4b7b0a-9a84-4b39-9739-914ea9c622fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ngrams(text,n):\n",
    "#     out=[]\n",
    "#     for i in range(len(text)-n+1):\n",
    "#         out.append(tuple(text[i:i+n]))\n",
    "#     return out\n",
    "# j_qs_bigram=[ngrams(i,2) for i in j_questions]\n",
    "# j_qs_bigram_flat = []\n",
    "# for i in j_qs_bigram:\n",
    "#     for j in i:\n",
    "#         j_qs_bigram_flat.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "98679d4c-f73c-4504-b5b0-2bdf5974e6b2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('for', 'the'),\n",
       " ('the', 'last'),\n",
       " ('last', '8'),\n",
       " ('8', 'years'),\n",
       " ('years', 'of'),\n",
       " ('of', 'his'),\n",
       " ('his', 'life'),\n",
       " ('life', ','),\n",
       " (',', 'galileo'),\n",
       " ('galileo', 'was'),\n",
       " ('was', 'under'),\n",
       " ('under', 'house'),\n",
       " ('house', 'arrest'),\n",
       " ('arrest', 'for'),\n",
       " ('for', 'espousing'),\n",
       " ('espousing', 'this'),\n",
       " ('this', \"man's\"),\n",
       " (\"man's\", 'theory'),\n",
       " ('no', '.'),\n",
       " ('.', '2'),\n",
       " ('2', ':'),\n",
       " (':', '1912'),\n",
       " ('1912', 'olympian'),\n",
       " ('olympian', ';'),\n",
       " (';', 'football')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# j_qs_bigram_flat[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9852c9a1-e3f6-4229-aef6-7e27e1eed192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ngram_frequency(ngram_list):\n",
    "#     frequency = {}\n",
    "#     for ngram in ngram_list:\n",
    "#         if ngram in frequency:\n",
    "#             frequency[ngram] += 1\n",
    "#         else:\n",
    "#             frequency[ngram] = 1\n",
    "# #    frequency = sorted(frequency.items(), key=lambda x:x[1], reverse=True)\n",
    "#     return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f818142f-090c-4744-bbfa-3ab13af6299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams = ngram_frequency(j_qs_bigram_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7100787b-aba4-4aaa-9ee1-eac9fe49aa3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 23153), (('of', 'this'), 19236), (('in', 'the'), 16028)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngrams[:0]\n",
    "# #x = list(ngrams.values())\n",
    "# #ngrams.sort()\n",
    "\n",
    "# list(ngrams)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a35b6c8f-16da-4de8-b44d-9e30297b8365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for t'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# questions_joined[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6654fb8d-168c-4590-b8eb-feba5a4eecb7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "\n",
    "class NgramModel():\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)        \n",
    "        \n",
    "    def _tokenize(self, corpus):\n",
    "        \n",
    "        tokenized_corpus = []\n",
    "        \n",
    "        # separate punctuation from previous word\n",
    "        spaced_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', corpus) \n",
    "        \n",
    "        # split into sentences\n",
    "        sentences = spaced_corpus.split('.')\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split() # split on whitespace\n",
    "            words = [word.lower() for word in words]\n",
    "            words = list(pad_both_ends(words, n=self.n))\n",
    "            tokenized_corpus += words\n",
    "        \n",
    "        return tokenized_corpus\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))    \n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "               \n",
    "        cfd = ConditionalFreqDist()\n",
    "        for ngram in self._ngrams:\n",
    "            condition = tuple(ngram[0:n-1]) \n",
    "            outcome = ngram[n-1]\n",
    "            \n",
    "            cfd[condition][outcome] += 1\n",
    "        bins = len(cfd) # we have to pass the number of bins in our freq dist in as a parameter to probability distribution, so we have a bin for every word\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, bins)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "    def generate(self, num_sentences = 1, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "        We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "        We can use a unigram model! \n",
    "        Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "        And how can we \n",
    "        Either way, we need a seed condition to enter into the loop with.\n",
    "        \"\"\"\n",
    "\n",
    "        # place to put generated tokens\n",
    "        string = []\n",
    "\n",
    "        if seed:\n",
    "            string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        else:\n",
    "            string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            next_token = tuple(string[-(self.n-1):])\n",
    "            \n",
    "            # keep generating tokens as long as we havent reached the stop sequence\n",
    "            while next_token != '</s>':\n",
    "                \n",
    "                # get the last n-1 tokens to condition on next\n",
    "                lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "    \n",
    "                next_token = self.cpd[lessgram].generate()\n",
    "                string.append( next_token )\n",
    "\n",
    "        string = ' '.join(string)\n",
    "\n",
    "        return string\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29fdb7bb-1dc6-45b2-84ed-dee63824639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NgramModel(questions_joined,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b839662-242a-4a98-8de4-85c656c56070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> <s> declare gov </s> </s> <s> <s> ludwig roselius\\' research into the heart with oxygen in 1988 is tv\\'s twinkie spokesman 1 of it term for reformers like wm </s> </s> <s> <s> female speed-skating gold medalist traded in one leap give up its sos & radioed back , situation room &, of course has his own star on her brother\\'s marriage this \"firebird\" composer\\'s opera \"rigoletto\" was titled \"ain\\'t misbehavin\\'\" in the college championship contestant margaret bickers , a deceived person is from webster groves , hills and fields , baby doc duvalier succeeded papa doc duvalier fled haiti a cylindrical package , & we could save the jews ; hey are known officially as the actor playing petruchio in \"the seduction of joe clark\\'s unusual tools in 2004 scarlett hit the longest ever of the sheet </s>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e831c3f8-8790-4ee1-aa26-547f6aa1f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# model_checkpoint = \"consciousAI/question-answering-roberta-base-s-v2\"\n",
    "\n",
    "# # context = \"\"\"\n",
    "# # 🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
    "# # between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "# # \"\"\"\n",
    "# # question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "\n",
    "# # question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
    "# # question_answerer(question=question, context=context)\n",
    "# question_answerer = pipeline(\"question-answering\",model=model_checkpoint)\n",
    "# question(answerer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdeab03-220b-41fb-b9e7-ea474fbfdbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc2a0d-68ed-4917-b7fb-976ba457aa23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
